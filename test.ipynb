{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%configure\n",
    "# { \"conf\": {\"spark.jars.packages\": 'spark.jars.packages, org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0,org.apache.spark:spark-avro_2.12:3.2.0' }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import findspark\n",
    "# findspark.init(\"/opt/spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2'\n",
    "# org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder.appName(\"test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"test\").config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0,org.apache.spark:spark-avro_2.12:3.2.0').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config\n",
    "#s3\n",
    "# s3_path = \"s3://emr-kafka-poc/\"\n",
    "output_file = \"simple_cluster_consumer.csv\"\n",
    "#spark\n",
    "spark_app_name = \"kafka_poc\"\n",
    "# kafka\n",
    "username = \"devqa\"\n",
    "password = \"p@ssw0rd\"\n",
    "msk_bootstrap = \"b-3.devqa-msk.4nvszg.c4.kafka.eu-west-1.amazonaws.com:9096,b-2.devqa-msk.4nvszg.c4.kafka.eu-west-1.amazonaws.com:9096,b-1.devqa-msk.4nvszg.c4.kafka.eu-west-1.amazonaws.com:9096\"\n",
    "topics = 'spark_testing'  # for testing, comma separate string for multiple topics\n",
    "kafka_group_id = \"spark_testing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of the records is 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "offset_ts = datetime.datetime(2022,1,12).timestamp()\n",
    "offset_string = {\n",
    "    topics : {\n",
    "        \"0\" : offset_ts,\n",
    "        \"1\" : offset_ts\n",
    "    }\n",
    "}\n",
    "\n",
    "offset_string = json.dumps(offset_string)\n",
    "\n",
    "options = {\n",
    "    \"kafka.bootstrap.servers\" : msk_bootstrap,\n",
    "    \"subscribe\" : topics,\n",
    "    # \"startingOffsetsByTimestamp\" : offset_string,\n",
    "    \"startingOffsets\" : \"earliest\",\n",
    "    \"kafka.sasl.jaas.config\": f'org.apache.kafka.common.security.scram.ScramLoginModule required username=\"{username}\" password=\"{password}\";',\n",
    "    \"kafka.sasl.mechanism\": \"SCRAM-SHA-512\",\n",
    "    \"kafka.security.protocol\" : \"SASL_SSL\",\n",
    "    \"group.id\": kafka_group_id\n",
    "}\n",
    "\n",
    "\n",
    "df = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .options(**options) \\\n",
    "    .load()\n",
    "print(f\"count of the records is {df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.option(\"sep\",\",\").csv(\"test/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import Row\n",
    "from subprocess import check_output\n",
    "\n",
    "\n",
    "myDF = spark.createDataFrame(\n",
    "    [\n",
    "        Row(col0=0, col1=1, col2=2),\n",
    "        Row(col0=3, col1=1, col2=5),\n",
    "        Row(col0=6, col1=2, col2=8),\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myGDF = myDF.select(\"*\").groupBy(\"col1\")\n",
    "myDF.createOrReplaceTempView(\"mydf_as_sqltable\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(col0=0, col1=1, col2=2), Row(col0=3, col1=1, col2=5), Row(col0=6, col1=2, col2=8)]\n"
     ]
    }
   ],
   "source": [
    "print(myDF.collect())\n",
    "# myGDF.sum().show()\n",
    "\n",
    "# spark.stop()\n",
    "# quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
